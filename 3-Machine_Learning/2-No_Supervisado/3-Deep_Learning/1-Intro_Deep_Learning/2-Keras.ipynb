{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos las imágenes de los dígitos asi como el conjunto de train y test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "60.000 imagenes de 28x28 pixeles\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego Nuñez\\AppData\\Local\\Temp\\ipykernel_444\\3096108358.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(255)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5019607843137255"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.000512393)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.6136101e-05, 2.7681663e-04, 2.7681663e-04, 2.7681663e-04,\n",
       "        1.9377163e-03, 2.0915035e-03, 2.6912726e-03, 3.9984621e-04,\n",
       "        2.5528644e-03, 3.9215689e-03, 3.7985391e-03, 1.9530950e-03,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.6136102e-04, 5.5363326e-04, 1.4455979e-03, 2.3683200e-03,\n",
       "        2.6143792e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 3.4602077e-03, 2.6451366e-03,\n",
       "        3.8908112e-03, 3.7216456e-03, 2.9988466e-03, 9.8423695e-04,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 7.5355632e-04,\n",
       "        3.6601308e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8600538e-03, 1.4302192e-03, 1.2610535e-03,\n",
       "        1.2610535e-03, 8.6120726e-04, 5.9976935e-04, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 2.7681663e-04,\n",
       "        3.3679355e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 3.0449827e-03, 2.7989235e-03,\n",
       "        3.7985391e-03, 3.7062669e-03, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.2302961e-03, 2.3990774e-03, 1.6455210e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.1526336e-03, 1.6916571e-04, 0.0000000e+00,\n",
       "        6.6128414e-04, 2.3683200e-03, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 2.1530181e-04, 1.5378702e-05, 2.3683200e-03,\n",
       "        3.8908112e-03, 1.3840831e-03, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 2.1376396e-03,\n",
       "        3.8908112e-03, 2.9219531e-03, 3.0757405e-05, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.6916571e-04,\n",
       "        2.9219531e-03, 3.8908112e-03, 1.0765091e-03, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        5.3825456e-04, 3.7062669e-03, 3.4602077e-03, 2.4605922e-03,\n",
       "        1.6608997e-03, 1.5378702e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.2456748e-03, 3.6908882e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 1.8300654e-03, 3.8446751e-04, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 6.9204153e-04, 2.8604383e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 2.3068052e-03, 4.1522493e-04,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 2.4605924e-04,\n",
       "        1.4302192e-03, 3.8754325e-03, 3.8908112e-03, 2.8758170e-03,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 3.8292964e-03, 3.8908112e-03, 3.8292964e-03,\n",
       "        9.8423695e-04, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 7.0742023e-04, 1.9992313e-03,\n",
       "        2.8143022e-03, 3.8908112e-03, 3.8908112e-03, 3.1833909e-03,\n",
       "        3.0757405e-05, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        5.9976935e-04, 2.2760478e-03, 3.5217225e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 3.8446751e-03, 2.7989235e-03,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 3.6908881e-04, 1.7531719e-03,\n",
       "        3.3986929e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.0911188e-03, 1.1995387e-03, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.5371011e-04, 1.0149943e-03, 3.2756634e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 3.8908112e-03, 3.0449827e-03,\n",
       "        1.2456748e-03, 3.0757405e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 2.7681663e-04, 2.6297579e-03,\n",
       "        3.3679355e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 2.9988466e-03, 1.2302961e-03, 1.3840832e-04,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        8.4582856e-04, 2.6451366e-03, 3.4755864e-03, 3.8908112e-03,\n",
       "        3.8908112e-03, 3.8908112e-03, 3.8908112e-03, 3.7524030e-03,\n",
       "        2.0453674e-03, 1.6916571e-04, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.0915035e-03, 3.8908112e-03, 3.8908112e-03, 3.8908112e-03,\n",
       "        3.2602844e-03, 2.0761248e-03, 2.0299887e-03, 2.4605924e-04,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comprobamos la normalización\n",
    "'''\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diego Nuñez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# Capa entrada\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 300,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units = 100,\n",
    "                            activation='relu'))\n",
    "\n",
    "# Capa salida\n",
    "model.add(keras.layers.Dense(units = 10,\n",
    "                            activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra manera de declarar la red neuronal\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dense name=dense, built=True>\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00347192, -0.02064667, -0.07147136, ...,  0.03177332,\n",
       "         0.06986657,  0.04713842],\n",
       "       [ 0.053525  , -0.07307564,  0.02251427, ...,  0.06029806,\n",
       "        -0.01432672,  0.02664495],\n",
       "       [ 0.00797854,  0.03747305, -0.05406775, ...,  0.06325604,\n",
       "        -0.03796376, -0.01809222],\n",
       "       ...,\n",
       "       [ 0.04622291,  0.06726463, -0.06869981, ..., -0.05035216,\n",
       "        -0.0478576 , -0.06177048],\n",
       "       [ 0.02380551,  0.06734397, -0.05479803, ...,  0.01731069,\n",
       "        -0.03707667, -0.06662432],\n",
       "       [ 0.03234879, -0.02865277, -0.02409487, ..., -0.0157722 ,\n",
       "         0.05853409,  0.07413259]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300*784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalente\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1524 - loss: 2.3010 - val_accuracy: 0.1064 - val_loss: 2.2999\n",
      "Epoch 2/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1129 - loss: 2.2993 - val_accuracy: 0.1064 - val_loss: 2.2997\n",
      "Epoch 3/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1123 - loss: 2.2991 - val_accuracy: 0.1064 - val_loss: 2.2997\n",
      "Epoch 4/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1126 - loss: 2.2988 - val_accuracy: 0.1064 - val_loss: 2.2996\n",
      "Epoch 5/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1132 - loss: 2.2988 - val_accuracy: 0.1064 - val_loss: 2.2995\n",
      "Epoch 6/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1140 - loss: 2.2986 - val_accuracy: 0.1064 - val_loss: 2.2993\n",
      "Epoch 7/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1133 - loss: 2.2985 - val_accuracy: 0.1064 - val_loss: 2.2993\n",
      "Epoch 8/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1137 - loss: 2.2985 - val_accuracy: 0.1064 - val_loss: 2.2991\n",
      "Epoch 9/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1132 - loss: 2.2984 - val_accuracy: 0.1064 - val_loss: 2.2991\n",
      "Epoch 10/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1131 - loss: 2.2984 - val_accuracy: 0.1064 - val_loss: 2.2990\n",
      "Epoch 11/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1151 - loss: 2.2979 - val_accuracy: 0.1064 - val_loss: 2.2989\n",
      "Epoch 12/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1121 - loss: 2.2982 - val_accuracy: 0.1064 - val_loss: 2.2989\n",
      "Epoch 13/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1128 - loss: 2.2981 - val_accuracy: 0.1064 - val_loss: 2.2987\n",
      "Epoch 14/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1151 - loss: 2.2977 - val_accuracy: 0.1064 - val_loss: 2.2985\n",
      "Epoch 15/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1138 - loss: 2.2979 - val_accuracy: 0.1064 - val_loss: 2.2985\n",
      "Epoch 16/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1126 - loss: 2.2975 - val_accuracy: 0.1064 - val_loss: 2.2984\n",
      "Epoch 17/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1134 - loss: 2.2977 - val_accuracy: 0.1064 - val_loss: 2.2982\n",
      "Epoch 18/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1114 - loss: 2.2975 - val_accuracy: 0.1064 - val_loss: 2.2981\n",
      "Epoch 19/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1146 - loss: 2.2970 - val_accuracy: 0.1064 - val_loss: 2.2980\n",
      "Epoch 20/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1132 - loss: 2.2971 - val_accuracy: 0.1064 - val_loss: 2.2979\n",
      "Epoch 21/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1160 - loss: 2.2968 - val_accuracy: 0.1064 - val_loss: 2.2977\n",
      "Epoch 22/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1125 - loss: 2.2971 - val_accuracy: 0.1064 - val_loss: 2.2976\n",
      "Epoch 23/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1146 - loss: 2.2965 - val_accuracy: 0.1064 - val_loss: 2.2975\n",
      "Epoch 24/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1142 - loss: 2.2966 - val_accuracy: 0.1064 - val_loss: 2.2973\n",
      "Epoch 25/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1133 - loss: 2.2966 - val_accuracy: 0.1064 - val_loss: 2.2972\n",
      "Epoch 26/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1136 - loss: 2.2964 - val_accuracy: 0.1064 - val_loss: 2.2970\n",
      "Epoch 27/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1150 - loss: 2.2960 - val_accuracy: 0.1064 - val_loss: 2.2969\n",
      "Epoch 28/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1109 - loss: 2.2962 - val_accuracy: 0.1064 - val_loss: 2.2968\n",
      "Epoch 29/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1134 - loss: 2.2959 - val_accuracy: 0.1064 - val_loss: 2.2966\n",
      "Epoch 30/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1129 - loss: 2.2958 - val_accuracy: 0.1064 - val_loss: 2.2965\n",
      "Epoch 31/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1134 - loss: 2.2956 - val_accuracy: 0.1064 - val_loss: 2.2963\n",
      "Epoch 32/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1136 - loss: 2.2957 - val_accuracy: 0.1064 - val_loss: 2.2961\n",
      "Epoch 33/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1136 - loss: 2.2949 - val_accuracy: 0.1064 - val_loss: 2.2960\n",
      "Epoch 34/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1135 - loss: 2.2953 - val_accuracy: 0.1064 - val_loss: 2.2957\n",
      "Epoch 35/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1121 - loss: 2.2952 - val_accuracy: 0.1064 - val_loss: 2.2956\n",
      "Epoch 36/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1123 - loss: 2.2950 - val_accuracy: 0.1064 - val_loss: 2.2954\n",
      "Epoch 37/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1112 - loss: 2.2947 - val_accuracy: 0.1064 - val_loss: 2.2952\n",
      "Epoch 38/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1152 - loss: 2.2942 - val_accuracy: 0.1064 - val_loss: 2.2950\n",
      "Epoch 39/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1141 - loss: 2.2941 - val_accuracy: 0.1064 - val_loss: 2.2948\n",
      "Epoch 40/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1110 - loss: 2.2946 - val_accuracy: 0.1064 - val_loss: 2.2947\n",
      "Epoch 41/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1136 - loss: 2.2939 - val_accuracy: 0.1064 - val_loss: 2.2944\n",
      "Epoch 42/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1156 - loss: 2.2934 - val_accuracy: 0.1064 - val_loss: 2.2942\n",
      "Epoch 43/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.1126 - loss: 2.2933 - val_accuracy: 0.1064 - val_loss: 2.2941\n",
      "Epoch 44/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1131 - loss: 2.2932 - val_accuracy: 0.1064 - val_loss: 2.2938\n",
      "Epoch 45/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1129 - loss: 2.2929 - val_accuracy: 0.1064 - val_loss: 2.2935\n",
      "Epoch 46/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1128 - loss: 2.2930 - val_accuracy: 0.1064 - val_loss: 2.2933\n",
      "Epoch 47/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1133 - loss: 2.2924 - val_accuracy: 0.1064 - val_loss: 2.2931\n",
      "Epoch 48/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1123 - loss: 2.2925 - val_accuracy: 0.1064 - val_loss: 2.2929\n",
      "Epoch 49/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1142 - loss: 2.2921 - val_accuracy: 0.1064 - val_loss: 2.2925\n",
      "Epoch 50/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1144 - loss: 2.2918 - val_accuracy: 0.1064 - val_loss: 2.2923\n"
     ]
    }
   ],
   "source": [
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(units = 300, activation='relu'),\n",
    "    keras.layers.Dense(units = 100, activation='relu'),\n",
    "    keras.layers.Dense(units = 10, activation='softmax')\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential(capas)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"sgd\",\n",
    "    loss = \"sparse_categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"], \n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784 * 300 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235500\n"
     ]
    }
   ],
   "source": [
    "# 1º neurona de la 1º hidden layer\n",
    "# y = a + w1*x1 + w2*x2 + .... wn*xn\n",
    "# a es el intercepto llamado bias\n",
    "# wn es cada uno de los pesos que va a ir actualizando con el backpropagation\n",
    "# n es 784\n",
    "# En la 1º hidden layer tenemos 784 pesos por cada neurona, al tener 300, tenemos un total de:\n",
    "print(784*300 + 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 784 + 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30100"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300 * 100 + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * 10 + 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312.5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5203 - loss: 1.6981 - val_accuracy: 0.8742 - val_loss: 0.5644\n",
      "Epoch 2/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8673 - loss: 0.5418 - val_accuracy: 0.8996 - val_loss: 0.3791\n",
      "Epoch 3/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.8924 - loss: 0.3983 - val_accuracy: 0.9105 - val_loss: 0.3224\n",
      "Epoch 4/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9059 - loss: 0.3411 - val_accuracy: 0.9176 - val_loss: 0.2944\n",
      "Epoch 5/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9111 - loss: 0.3158 - val_accuracy: 0.9216 - val_loss: 0.2761\n",
      "Epoch 6/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9155 - loss: 0.2975 - val_accuracy: 0.9272 - val_loss: 0.2584\n",
      "Epoch 7/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9238 - loss: 0.2710 - val_accuracy: 0.9311 - val_loss: 0.2469\n",
      "Epoch 8/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9271 - loss: 0.2591 - val_accuracy: 0.9334 - val_loss: 0.2362\n",
      "Epoch 9/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9295 - loss: 0.2514 - val_accuracy: 0.9361 - val_loss: 0.2269\n",
      "Epoch 10/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9324 - loss: 0.2389 - val_accuracy: 0.9373 - val_loss: 0.2195\n",
      "Epoch 11/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9341 - loss: 0.2301 - val_accuracy: 0.9396 - val_loss: 0.2110\n",
      "Epoch 12/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9374 - loss: 0.2222 - val_accuracy: 0.9431 - val_loss: 0.2043\n",
      "Epoch 13/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9399 - loss: 0.2088 - val_accuracy: 0.9443 - val_loss: 0.1990\n",
      "Epoch 14/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9425 - loss: 0.2034 - val_accuracy: 0.9474 - val_loss: 0.1928\n",
      "Epoch 15/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9447 - loss: 0.1963 - val_accuracy: 0.9475 - val_loss: 0.1881\n",
      "Epoch 16/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9459 - loss: 0.1897 - val_accuracy: 0.9495 - val_loss: 0.1833\n",
      "Epoch 17/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9481 - loss: 0.1852 - val_accuracy: 0.9528 - val_loss: 0.1793\n",
      "Epoch 18/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9488 - loss: 0.1773 - val_accuracy: 0.9531 - val_loss: 0.1754\n",
      "Epoch 19/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9512 - loss: 0.1711 - val_accuracy: 0.9542 - val_loss: 0.1714\n",
      "Epoch 20/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9531 - loss: 0.1646 - val_accuracy: 0.9552 - val_loss: 0.1667\n",
      "Epoch 21/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9535 - loss: 0.1622 - val_accuracy: 0.9556 - val_loss: 0.1625\n",
      "Epoch 22/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9550 - loss: 0.1606 - val_accuracy: 0.9579 - val_loss: 0.1585\n",
      "Epoch 23/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9556 - loss: 0.1568 - val_accuracy: 0.9565 - val_loss: 0.1559\n",
      "Epoch 24/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9573 - loss: 0.1490 - val_accuracy: 0.9580 - val_loss: 0.1529\n",
      "Epoch 25/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9581 - loss: 0.1494 - val_accuracy: 0.9583 - val_loss: 0.1510\n",
      "Epoch 26/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9602 - loss: 0.1435 - val_accuracy: 0.9587 - val_loss: 0.1462\n",
      "Epoch 27/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9609 - loss: 0.1376 - val_accuracy: 0.9600 - val_loss: 0.1449\n",
      "Epoch 28/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9605 - loss: 0.1377 - val_accuracy: 0.9601 - val_loss: 0.1413\n",
      "Epoch 29/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9606 - loss: 0.1389 - val_accuracy: 0.9610 - val_loss: 0.1397\n",
      "Epoch 30/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9642 - loss: 0.1276 - val_accuracy: 0.9612 - val_loss: 0.1375\n",
      "Epoch 31/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9657 - loss: 0.1247 - val_accuracy: 0.9613 - val_loss: 0.1371\n",
      "Epoch 32/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9639 - loss: 0.1256 - val_accuracy: 0.9627 - val_loss: 0.1335\n",
      "Epoch 33/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9654 - loss: 0.1245 - val_accuracy: 0.9627 - val_loss: 0.1329\n",
      "Epoch 34/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9667 - loss: 0.1175 - val_accuracy: 0.9635 - val_loss: 0.1296\n",
      "Epoch 35/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9675 - loss: 0.1147 - val_accuracy: 0.9638 - val_loss: 0.1274\n",
      "Epoch 36/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9687 - loss: 0.1137 - val_accuracy: 0.9644 - val_loss: 0.1274\n",
      "Epoch 37/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9687 - loss: 0.1111 - val_accuracy: 0.9652 - val_loss: 0.1244\n",
      "Epoch 38/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9695 - loss: 0.1091 - val_accuracy: 0.9656 - val_loss: 0.1237\n",
      "Epoch 39/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9713 - loss: 0.1029 - val_accuracy: 0.9650 - val_loss: 0.1238\n",
      "Epoch 40/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9715 - loss: 0.1023 - val_accuracy: 0.9656 - val_loss: 0.1202\n",
      "Epoch 41/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9721 - loss: 0.1002 - val_accuracy: 0.9667 - val_loss: 0.1193\n",
      "Epoch 42/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9713 - loss: 0.1027 - val_accuracy: 0.9667 - val_loss: 0.1178\n",
      "Epoch 43/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9731 - loss: 0.0970 - val_accuracy: 0.9672 - val_loss: 0.1160\n",
      "Epoch 44/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9745 - loss: 0.0931 - val_accuracy: 0.9679 - val_loss: 0.1143\n",
      "Epoch 45/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9731 - loss: 0.0954 - val_accuracy: 0.9676 - val_loss: 0.1134\n",
      "Epoch 46/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9736 - loss: 0.0952 - val_accuracy: 0.9676 - val_loss: 0.1126\n",
      "Epoch 47/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9750 - loss: 0.0895 - val_accuracy: 0.9680 - val_loss: 0.1116\n",
      "Epoch 48/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9765 - loss: 0.0868 - val_accuracy: 0.9690 - val_loss: 0.1106\n",
      "Epoch 49/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9768 - loss: 0.0858 - val_accuracy: 0.9695 - val_loss: 0.1093\n",
      "Epoch 50/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.9762 - loss: 0.0860 - val_accuracy: 0.9696 - val_loss: 0.1081\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 50,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1140 - loss: 2.2916 - val_accuracy: 0.1064 - val_loss: 2.2919\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1167 - loss: 2.2905 - val_accuracy: 0.1064 - val_loss: 2.2911\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1127 - loss: 2.2907 - val_accuracy: 0.1064 - val_loss: 2.2907\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1141 - loss: 2.2897 - val_accuracy: 0.1064 - val_loss: 2.2900\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1121 - loss: 2.2893 - val_accuracy: 0.1064 - val_loss: 2.2894\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1130 - loss: 2.2887 - val_accuracy: 0.1064 - val_loss: 2.2886\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1129 - loss: 2.2878 - val_accuracy: 0.1064 - val_loss: 2.2876\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1129 - loss: 2.2865 - val_accuracy: 0.1064 - val_loss: 2.2870\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1147 - loss: 2.2858 - val_accuracy: 0.1075 - val_loss: 2.2855\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.1194 - loss: 2.2844 - val_accuracy: 0.1198 - val_loss: 2.2847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21d3712df50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs = 10,\n",
    "    validation_data = (X_val, y_val) # validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.1292400062084198, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271, 0.11355999857187271], 'loss': [2.300344705581665, 2.299337387084961, 2.299039602279663, 2.2988765239715576, 2.298759937286377, 2.298628568649292, 2.2985587120056152, 2.2984516620635986, 2.298358201980591, 2.298264265060425, 2.2981464862823486, 2.298058032989502, 2.2979507446289062, 2.2978389263153076, 2.2977283000946045, 2.2976315021514893, 2.2974865436553955, 2.297368288040161, 2.29728102684021, 2.2971625328063965, 2.2970330715179443, 2.29691743850708, 2.2967920303344727, 2.296661615371704, 2.296525239944458, 2.2963931560516357, 2.2962441444396973, 2.2960922718048096, 2.2959606647491455, 2.2958059310913086, 2.2956480979919434, 2.295503854751587, 2.2953171730041504, 2.295170545578003, 2.295001745223999, 2.2948315143585205, 2.294633388519287, 2.294442653656006, 2.294281482696533, 2.2940521240234375, 2.293896198272705, 2.293667793273926, 2.2934627532958984, 2.2932703495025635, 2.2930262088775635, 2.2928030490875244, 2.2925808429718018, 2.2923214435577393, 2.292083978652954, 2.2918310165405273], 'val_accuracy': [0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951, 0.10639999806880951], 'val_loss': [2.2999277114868164, 2.2997219562530518, 2.2996699810028076, 2.2995736598968506, 2.299503803253174, 2.29934024810791, 2.299257278442383, 2.299119472503662, 2.2990806102752686, 2.2989704608917236, 2.2988924980163574, 2.2988603115081787, 2.2986838817596436, 2.298482894897461, 2.298478364944458, 2.2984015941619873, 2.2981934547424316, 2.2981228828430176, 2.297961473464966, 2.2978761196136475, 2.2976932525634766, 2.2975523471832275, 2.2974603176116943, 2.2973082065582275, 2.297161817550659, 2.29699969291687, 2.2969179153442383, 2.2968480587005615, 2.296576976776123, 2.2964890003204346, 2.2962613105773926, 2.2960917949676514, 2.2959835529327393, 2.2957332134246826, 2.2955760955810547, 2.295441150665283, 2.2952163219451904, 2.29495906829834, 2.2948124408721924, 2.294651746749878, 2.294407367706299, 2.294231653213501, 2.2940685749053955, 2.2938268184661865, 2.2934839725494385, 2.293335437774658, 2.293090343475342, 2.292882204055786, 2.2924740314483643, 2.2922537326812744]}\n"
     ]
    }
   ],
   "source": [
    "# print(history.params)\n",
    "# print(history.epoch)\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.1292400062084198,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271,\n",
       "  0.11355999857187271],\n",
       " 'loss': [2.300344705581665,\n",
       "  2.299337387084961,\n",
       "  2.299039602279663,\n",
       "  2.2988765239715576,\n",
       "  2.298759937286377,\n",
       "  2.298628568649292,\n",
       "  2.2985587120056152,\n",
       "  2.2984516620635986,\n",
       "  2.298358201980591,\n",
       "  2.298264265060425,\n",
       "  2.2981464862823486,\n",
       "  2.298058032989502,\n",
       "  2.2979507446289062,\n",
       "  2.2978389263153076,\n",
       "  2.2977283000946045,\n",
       "  2.2976315021514893,\n",
       "  2.2974865436553955,\n",
       "  2.297368288040161,\n",
       "  2.29728102684021,\n",
       "  2.2971625328063965,\n",
       "  2.2970330715179443,\n",
       "  2.29691743850708,\n",
       "  2.2967920303344727,\n",
       "  2.296661615371704,\n",
       "  2.296525239944458,\n",
       "  2.2963931560516357,\n",
       "  2.2962441444396973,\n",
       "  2.2960922718048096,\n",
       "  2.2959606647491455,\n",
       "  2.2958059310913086,\n",
       "  2.2956480979919434,\n",
       "  2.295503854751587,\n",
       "  2.2953171730041504,\n",
       "  2.295170545578003,\n",
       "  2.295001745223999,\n",
       "  2.2948315143585205,\n",
       "  2.294633388519287,\n",
       "  2.294442653656006,\n",
       "  2.294281482696533,\n",
       "  2.2940521240234375,\n",
       "  2.293896198272705,\n",
       "  2.293667793273926,\n",
       "  2.2934627532958984,\n",
       "  2.2932703495025635,\n",
       "  2.2930262088775635,\n",
       "  2.2928030490875244,\n",
       "  2.2925808429718018,\n",
       "  2.2923214435577393,\n",
       "  2.292083978652954,\n",
       "  2.2918310165405273],\n",
       " 'val_accuracy': [0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951,\n",
       "  0.10639999806880951],\n",
       " 'val_loss': [2.2999277114868164,\n",
       "  2.2997219562530518,\n",
       "  2.2996699810028076,\n",
       "  2.2995736598968506,\n",
       "  2.299503803253174,\n",
       "  2.29934024810791,\n",
       "  2.299257278442383,\n",
       "  2.299119472503662,\n",
       "  2.2990806102752686,\n",
       "  2.2989704608917236,\n",
       "  2.2988924980163574,\n",
       "  2.2988603115081787,\n",
       "  2.2986838817596436,\n",
       "  2.298482894897461,\n",
       "  2.298478364944458,\n",
       "  2.2984015941619873,\n",
       "  2.2981934547424316,\n",
       "  2.2981228828430176,\n",
       "  2.297961473464966,\n",
       "  2.2978761196136475,\n",
       "  2.2976932525634766,\n",
       "  2.2975523471832275,\n",
       "  2.2974603176116943,\n",
       "  2.2973082065582275,\n",
       "  2.297161817550659,\n",
       "  2.29699969291687,\n",
       "  2.2969179153442383,\n",
       "  2.2968480587005615,\n",
       "  2.296576976776123,\n",
       "  2.2964890003204346,\n",
       "  2.2962613105773926,\n",
       "  2.2960917949676514,\n",
       "  2.2959835529327393,\n",
       "  2.2957332134246826,\n",
       "  2.2955760955810547,\n",
       "  2.295441150665283,\n",
       "  2.2952163219451904,\n",
       "  2.29495906829834,\n",
       "  2.2948124408721924,\n",
       "  2.294651746749878,\n",
       "  2.294407367706299,\n",
       "  2.294231653213501,\n",
       "  2.2940685749053955,\n",
       "  2.2938268184661865,\n",
       "  2.2934839725494385,\n",
       "  2.293335437774658,\n",
       "  2.293090343475342,\n",
       "  2.292882204055786,\n",
       "  2.2924740314483643,\n",
       "  2.2922537326812744]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12924</td>\n",
       "      <td>2.300345</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.299337</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.299040</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298877</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298760</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298629</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298559</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298452</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298358</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.299081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298264</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298146</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.298058</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297951</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297839</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297728</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297632</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297487</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297368</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.298123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297281</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297163</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.297033</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296917</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296792</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296662</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296525</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296393</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296244</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.296918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.296092</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.296848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295961</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.296577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295806</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.296489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295648</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.296261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295504</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.296092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295317</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.295984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295171</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.295733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.295002</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.295576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.294832</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.295441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.294633</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.295216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.294443</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.294959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.294281</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.294812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.294052</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.294652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.293896</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.294407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.293668</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.294232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.293463</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.294069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.293270</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.293827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.293026</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.293484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.292803</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.293335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.292581</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.293090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.292321</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.292882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.292084</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.292474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.11356</td>\n",
       "      <td>2.291831</td>\n",
       "      <td>0.1064</td>\n",
       "      <td>2.292254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy      loss  val_accuracy  val_loss\n",
       "0    0.12924  2.300345        0.1064  2.299928\n",
       "1    0.11356  2.299337        0.1064  2.299722\n",
       "2    0.11356  2.299040        0.1064  2.299670\n",
       "3    0.11356  2.298877        0.1064  2.299574\n",
       "4    0.11356  2.298760        0.1064  2.299504\n",
       "5    0.11356  2.298629        0.1064  2.299340\n",
       "6    0.11356  2.298559        0.1064  2.299257\n",
       "7    0.11356  2.298452        0.1064  2.299119\n",
       "8    0.11356  2.298358        0.1064  2.299081\n",
       "9    0.11356  2.298264        0.1064  2.298970\n",
       "10   0.11356  2.298146        0.1064  2.298892\n",
       "11   0.11356  2.298058        0.1064  2.298860\n",
       "12   0.11356  2.297951        0.1064  2.298684\n",
       "13   0.11356  2.297839        0.1064  2.298483\n",
       "14   0.11356  2.297728        0.1064  2.298478\n",
       "15   0.11356  2.297632        0.1064  2.298402\n",
       "16   0.11356  2.297487        0.1064  2.298193\n",
       "17   0.11356  2.297368        0.1064  2.298123\n",
       "18   0.11356  2.297281        0.1064  2.297961\n",
       "19   0.11356  2.297163        0.1064  2.297876\n",
       "20   0.11356  2.297033        0.1064  2.297693\n",
       "21   0.11356  2.296917        0.1064  2.297552\n",
       "22   0.11356  2.296792        0.1064  2.297460\n",
       "23   0.11356  2.296662        0.1064  2.297308\n",
       "24   0.11356  2.296525        0.1064  2.297162\n",
       "25   0.11356  2.296393        0.1064  2.297000\n",
       "26   0.11356  2.296244        0.1064  2.296918\n",
       "27   0.11356  2.296092        0.1064  2.296848\n",
       "28   0.11356  2.295961        0.1064  2.296577\n",
       "29   0.11356  2.295806        0.1064  2.296489\n",
       "30   0.11356  2.295648        0.1064  2.296261\n",
       "31   0.11356  2.295504        0.1064  2.296092\n",
       "32   0.11356  2.295317        0.1064  2.295984\n",
       "33   0.11356  2.295171        0.1064  2.295733\n",
       "34   0.11356  2.295002        0.1064  2.295576\n",
       "35   0.11356  2.294832        0.1064  2.295441\n",
       "36   0.11356  2.294633        0.1064  2.295216\n",
       "37   0.11356  2.294443        0.1064  2.294959\n",
       "38   0.11356  2.294281        0.1064  2.294812\n",
       "39   0.11356  2.294052        0.1064  2.294652\n",
       "40   0.11356  2.293896        0.1064  2.294407\n",
       "41   0.11356  2.293668        0.1064  2.294232\n",
       "42   0.11356  2.293463        0.1064  2.294069\n",
       "43   0.11356  2.293270        0.1064  2.293827\n",
       "44   0.11356  2.293026        0.1064  2.293484\n",
       "45   0.11356  2.292803        0.1064  2.293335\n",
       "46   0.11356  2.292581        0.1064  2.293090\n",
       "47   0.11356  2.292321        0.1064  2.292882\n",
       "48   0.11356  2.292084        0.1064  2.292474\n",
       "49   0.11356  2.291831        0.1064  2.292254"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1265 - loss: 2.2855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.284050703048706, 0.13230000436306]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego Nuñez\\AppData\\Local\\Temp\\ipykernel_15952\\1468152043.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cogemos el primero\n",
    "plt.imshow(X_test[0].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.098, 0.109, 0.098, 0.1  , 0.097, 0.092, 0.101, 0.107, 0.096,\n",
       "        0.102]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test[:1])\n",
    "print(predictions.shape)\n",
    "np.round(predictions,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0983573 , 0.10916872, 0.09751526, 0.09973376, 0.09662934,\n",
       "        0.09225365, 0.10116245, 0.10726269, 0.09586563, 0.10205124]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312.5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[0]/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Diego Nuñez\\AppData\\Local\\Temp\\ipykernel_15952\\4029188365.py:1: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  plt.imshow(X_test[1].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[1].reshape(28,28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  70,  833,    0,    0,    0,    0,   77,    0,    0,    0],\n",
       "       [   0, 1135,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1030,    0,    0,    0,    0,    2,    0,    0,    0],\n",
       "       [   0, 1010,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,  980,    0,    0,    0,    0,    2,    0,    0,    0],\n",
       "       [   0,  892,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0,  878,    0,    0,    0,    0,   80,    0,    0,    0],\n",
       "       [   0,  990,    0,    0,    0,    0,    0,   38,    0,    0],\n",
       "       [   0,  974,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1008,    0,    0,    0,    0,    1,    0,    0,    0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix(y_test, model.predict(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       980\n",
      "         1.0       0.98      0.99      0.99      1135\n",
      "         2.0       0.97      0.97      0.97      1032\n",
      "         3.0       0.96      0.98      0.97      1010\n",
      "         4.0       0.98      0.97      0.97       982\n",
      "         5.0       0.98      0.97      0.97       892\n",
      "         6.0       0.98      0.97      0.97       958\n",
      "         7.0       0.98      0.97      0.97      1028\n",
      "         8.0       0.96      0.97      0.96       974\n",
      "         9.0       0.97      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test).argmax(axis=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos datos\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data,\n",
    "                                                              housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,\n",
    "                                                      y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362.8125"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11610/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diego Nuñez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8201 - val_loss: 0.5299\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5177 - val_loss: 0.4643\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4619 - val_loss: 0.4324\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4295 - val_loss: 0.4210\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4291 - val_loss: 0.4164\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4260 - val_loss: 0.3982\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4150 - val_loss: 0.3858\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3936 - val_loss: 0.3858\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3836 - val_loss: 0.3767\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4110 - val_loss: 0.3717\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3880 - val_loss: 0.3682\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3687 - val_loss: 0.3685\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3938 - val_loss: 0.3903\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4130 - val_loss: 0.3802\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3789 - val_loss: 0.3614\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3733 - val_loss: 0.3649\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3808 - val_loss: 0.3598\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3710 - val_loss: 0.3565\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3769 - val_loss: 0.3609\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3784 - val_loss: 0.3512\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu',\n",
    "                      input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "             optimizer = \"sgd\")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs = 20,\n",
    "                   validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*30 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │           \u001b[38;5;34m270\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> (1.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m303\u001b[0m (1.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m301\u001b[0m (1.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3689\n",
      "0.3622754216194153\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3760054],\n",
       "       [1.2083163],\n",
       "       [2.072228 ],\n",
       "       [0.6865401],\n",
       "       [1.1529279]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Lo volvemos a cargar\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m checkpoint_cb = keras.callbacks.ModelCheckpoint(\u001b[33m\"\u001b[39m\u001b[33mcallback_model.h5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                   \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Diego Nuñez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Diego Nuñez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:171\u001b[39m, in \u001b[36mconvert_to_numpy\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf.RaggedTensor):\n\u001b[32m    170\u001b[39m     x = x.to_tensor()\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(x)\n",
      "\u001b[31mNotImplementedError\u001b[39m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=30,\n",
    "                   callbacks = [checkpoint_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m early_stopping_cb = keras.callbacks.EarlyStopping(patience=\u001b[32m3\u001b[39m)\n\u001b[32m      2\u001b[39m history = model.fit(X_train,\n\u001b[32m      3\u001b[39m                    y_train,\n\u001b[32m      4\u001b[39m                    epochs=\u001b[32m50\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m                    validation_data = (\u001b[43mX_valid\u001b[49m, y_valid),\n\u001b[32m      6\u001b[39m                    callbacks = [early_stopping_cb, checkpoint_cb])\n",
      "\u001b[31mNameError\u001b[39m: name 'X_valid' is not defined"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                   validation_data = (X_valid, y_valid),\n",
    "                   callbacks = [early_stopping_cb, checkpoint_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
